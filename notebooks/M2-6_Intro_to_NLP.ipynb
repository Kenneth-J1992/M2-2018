{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# Natural Language Processing (NLP) in Python\n",
    "## An introduction to key concepts and techniques\n",
    "\n",
    "In this notebook we are going to explore central concepts of NLP and their implementation in modern high-level Python libraries.\n",
    "This is aimed to be a very general introduction to make this field more approacheable and also provide some familiarity with the specific jargon. While NLP offers many opportunities as a technique (or actually an array of different techniques) for social science research the application is yet limited but growing.\n",
    "\n",
    "The research field of NLP itself has been turn upside-down and developed a lot since the introduction of word embeddings around 2013 and the growth of deep learning (neural network models) in the past 3-4 years. Particularly recurrent neural networks and the LSTM (Long short-term memory) variation shifted the research field.\n",
    "\n",
    "![nlp problems](https://image.slidesharecdn.com/lang-detect-161011092815/95/nlp-project-full-cycle-16-638.jpg)\n",
    "\n",
    "\n",
    "This workshop aims at presenting established techniques that I think are most useful in a social science research setting.\n",
    "\n",
    "To be more specific, below we will explore:\n",
    "\n",
    "- basic string manipulation\n",
    "- tokens and tokenization + some preprocessing\n",
    "- the Bag-of-Words model\n",
    "- topic modeling (and its close relation to dimensionality reduction / unsupervised machine learning)\n",
    "- entity extraction\n",
    "- text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8e17b1aa00a18ee6658bbbebed214f55545bb2fe"
   },
   "source": [
    "### Basic string manipulation and tokenization\n",
    "\n",
    "In the following we will just juse basic python string manipulation\n",
    "\n",
    "You can do much much more if you learn using regular expressions (RegEx) but that would go too far - and you can learn some of it in the DC course.\n",
    "\n",
    "Let's start with a recent news text form the Guardian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bc74dc66168ce914fb2fa57030be8d04c5f8c786"
   },
   "outputs": [],
   "source": [
    "text = \"\"\"The US Senate has voted to confirm judge Brett Kavanaugh to the supreme court, handing Donald Trump a major victory and America a bench expected to tilt to the right for the next generation.\n",
    "The president will hold a ceremony for Kavanaugh at the White House on Monday evening and he is expected to take his place on the court on Tuesday.\n",
    "After a bitter fight on Capitol Hill dominated by partisan entrenchment and the allegations of sexual assault against Kavanaugh, the 53-year-old federal judge was sworn in by supreme court chief justice John Roberts on Saturday evening just a few hours after Republicans won the confirmation vote 50 to 48.\n",
    "Furious protesters hammered on the huge front doors beneath the white columns of the majestic court building on Capitol Hill as Kavanaugh was being sworn in, following a day of demonstrations that saw many arrested but were more muted than days earlier, as it became clear the ultra-conservative’s confirmation was all but inevitable.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0004a9e91bad4f3b6acdfcf8421a38407f1c4fda"
   },
   "outputs": [],
   "source": [
    "# We can split the text-chunk into something like sentences.\n",
    "split_text = text.split('.')\n",
    "print(split_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "82694ae57e7d801b0d83579e01756495efba4dc4"
   },
   "outputs": [],
   "source": [
    "# print out the first stentence\n",
    "sentence_3 = split_text[2]\n",
    "print(sentence_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "52ce4d42879932296721af6bf57d2440d1e3c628"
   },
   "outputs": [],
   "source": [
    "# Let's create tokens\n",
    "tokens_sentence_3 = [word for word in sentence_3.split(' ')]\n",
    "print(tokens_sentence_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "53bfd1e2120868b423c0c4a99ac64f4a8634bd2f"
   },
   "outputs": [],
   "source": [
    "# Let's lowercase all these tokens and clean up the \\n (new line command)\n",
    "# Also we will replace \"()\" as well as make sure that only words lend in our list\n",
    "tokens_sentence_3_lower = [word.lower().strip() for word in sentence_3.split(' ')]\n",
    "print('### OUTPUT1 ###')\n",
    "print(tokens_sentence_3_lower)\n",
    "print('\\n')\n",
    "    \n",
    "tokens_sentence_3_lower = [word.replace('(','').replace(')','') \n",
    "                           for word in tokens_sentence_3_lower if word.isalpha()]\n",
    "\n",
    "print('### OUTPUT2 ###')\n",
    "print(tokens_sentence_3_lower)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f7a13cd3b9cbe888d821381c6fee88684121c077"
   },
   "outputs": [],
   "source": [
    "# Removing stopwords\n",
    "\n",
    "stopwords_en = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', \n",
    "                'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \n",
    "                \"you'd\", 'your', 'yours', 'yourself', 'yourselves', \n",
    "                'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', \n",
    "                'hers', 'herself', 'it', \"it's\", 'its', 'itself', \n",
    "                'they', 'them', 'their', 'theirs', 'themselves', 'what', \n",
    "                'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', \n",
    "                'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', \n",
    "                'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', \n",
    "                'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', \n",
    "                'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', \n",
    "                'between', 'into', 'through', 'during', 'before', 'after', 'above', \n",
    "                'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', \n",
    "                'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', \n",
    "                'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', \n",
    "                'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', \n",
    "                'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', \n",
    "                'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', \n",
    "                'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \n",
    "                \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \n",
    "                \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", \n",
    "                'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \n",
    "                'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "742adfae8302f83863195cafbc146a3b9b17c913"
   },
   "outputs": [],
   "source": [
    "tokens_sentence_3_clean = [word for word in tokens_sentence_3_lower if word not in stopwords_en]\n",
    "print(tokens_sentence_3_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3f145224681c353d8a3331c93746d9e645b77e16"
   },
   "source": [
    "Introducing NLTK, which will make your life much easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c1af2c20a5cb4034e72d58ff2987081411f39804"
   },
   "outputs": [],
   "source": [
    "# Tokenizing sentences\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Tokenizing words\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenizing Tweets!\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9615cbbca8db92f640e15175d22066042e25f117"
   },
   "outputs": [],
   "source": [
    "# Let's get our stences.\n",
    "# Note that the full-stops at the end of each sentence are still there\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "873bb1337316eb0444aa1056376b0c34ecc427af"
   },
   "outputs": [],
   "source": [
    "# Use word_tokenize to tokenize the third sentence: tokenized_sent\n",
    "tokenized_sent = word_tokenize(sentences[2])\n",
    "\n",
    "# Make a set of unique tokens in the entire scene: unique_tokens\n",
    "unique_tokens = set(word_tokenize(text))\n",
    "print(unique_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "415778a637cdc9c044b1bc96a7d8fad01a9b513c"
   },
   "source": [
    "Let's see how this works with teweets using a well known example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3fa33d081df8cde687fa67d06a5375185830f227"
   },
   "outputs": [],
   "source": [
    "tweets = [\"On behalf of @FLOTUS Melania & myself, THANK YOU for today's update & GREAT WORK! #SouthernBaptist @SendRelief,… https://t.co/4yZCeXCt6n\",\n",
    "\"I will be going to Texas and Louisiana tomorrow with First Lady. Great progress being made! Spending weekend working at White House.\",\n",
    "\"Stock Market up 5 months in a row!\",\n",
    "\"'President Donald J. Trump Proclaims September 3, 2017, as a National Day of Prayer' #HurricaneHarvey #PrayForTexas… https://t.co/tOMfFWwEsN\",\n",
    "\"Texas is healing fast thanks to all of the great men & women who have been working so hard. But still so much to do. Will be back tomorrow!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4d69a001609cec5a10b954b39326aa423b7c7a63"
   },
   "outputs": [],
   "source": [
    "# We can use the tweet tokenizer to parse these tweets:\n",
    "\n",
    "tknzr = TweetTokenizer()\n",
    "tweets_tokenized = [tknzr.tokenize(tweet) for tweet in tweets]\n",
    "print(tweets_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "681324b86db3e7550959539dc6de059e82151685"
   },
   "outputs": [],
   "source": [
    "# Get out all hashtags using loops\n",
    "\n",
    "hashtags = []\n",
    "\n",
    "for tweet in tweets_tokenized:\n",
    "    hashtags.extend([word for word in tweet if word.startswith('#')])\n",
    "    \n",
    "print(hashtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f20c9237885e1dff42c95e37fc916619faa95f64"
   },
   "source": [
    "### Bag of words model\n",
    "\n",
    "In order for a computer to understand text we need to somehow find a useful representation.\n",
    "If you need to compare different texts e.g. articles, you will probably go for keywords. These keywords may come from a keyword-list with for example 200 different keywords\n",
    "In that case you could represent each document with a (sparse) vector with 1 for \"keyword present\" and 0 for \"keyword absent\"\n",
    "We can also get a bit more sophoistocated and count the number of times a word from our dictionary occurs.\n",
    "For a corpus of documents that would give us a document-term matrix\n",
    "![example](https://i.stack.imgur.com/C1UMs.png)\n",
    "\n",
    "Let's try creating a bag of words model from our initial example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "946f4850235dc71315879fe69026fc8158404652"
   },
   "outputs": [],
   "source": [
    "# We import the Counter module from python's standard collections\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "word_tokenized = word_tokenize(text)\n",
    "bow = Counter(word_tokenized)\n",
    "print(bow.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9ae799eb5c79502810d6edce2a8d3a38b75f3cd9"
   },
   "outputs": [],
   "source": [
    "# Let's add some preprocessing\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "word_tokenized = word_tokenize(text)\n",
    "\n",
    "# lowercasing\n",
    "cleaned_word_tokenized = [word.lower().strip() for word in word_tokenized]\n",
    "# replacing some unwanted things\n",
    "cleaned_word_tokenized = [word.replace('(','').replace(')','') for word in cleaned_word_tokenized if word.isalpha()]\n",
    "# removing stopwords\n",
    "cleaned_word_tokenized = [word for word in cleaned_word_tokenized if word not in english_stopwords]\n",
    "\n",
    "bow = Counter(cleaned_word_tokenized)\n",
    "print(bow.most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7e465a11f405a9bb32c5b5d612d9431abc71509d"
   },
   "source": [
    "One important part of text preprocessing is normalization. Here we can use stemmers and lematizers to aggregate plural forms and similar. This can be extremely useful if working with languages that have a rich morphology such as Russian or Turkish.\n",
    "\n",
    "![example_stemm](https://image.slidesharecdn.com/lightweightnaturallanguageprocessingnlp-120314154200-phpapp01/95/lightweight-natural-language-processing-nlp-34-728.jpg?cb=1331814243)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e806c007d26ca17e3d9c7c257f31154ce1fc8d0e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's import a lemmatizer from NLTK and try how it works\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in cleaned_word_tokenized]\n",
    "\n",
    "# Create the bag-of-words: bow\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "59e83938cc1bcf80bb9a8d7f47c793cf73d948b1"
   },
   "source": [
    "So far you learned some basic unicode string manipulation and I also introduced NLTK. If you want to lean more about traditional NLP, check out the [free online book on NLTK](https://www.nltk.org/book/). You will learn old school NLP along with Python (and general programming foundations).\n",
    "\n",
    "When it comes to comparing documents (this is often what we want), simple \"keyword counts\" may be too simplistic and sure, we can do better – we can do topic modeling. One amazing library for working with state of the art topic models is Gensim.\n",
    "\n",
    "![gensim](https://rare-technologies.com/wp-content/uploads/2017/01/atmodel_plot-855x645.png)\n",
    "\n",
    "Let's try to work with a bigger dataset.\n",
    "\n",
    "Gensim allows you to work with a large number of high-performant NLP models including word embedding techniques.  We will be using something more traditional: TF-IDF and LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "69d7ef374f6b77f85d3f3b82072731a4ab3e2af6"
   },
   "outputs": [],
   "source": [
    "# We start by importing the data, ~1900 Abstracts/Titles from Scopus\n",
    "import pandas as pd\n",
    "\n",
    "abstracts = pd.read_csv('https://github.com/SDS-AAU/M2-2018/raw/master/input/abstracts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d1274c7a1c92683cfc9549bc5a71c260d8c13e05"
   },
   "outputs": [],
   "source": [
    "# Let's inspect the data\n",
    "abstracts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a51610b70e70d7610f281097d69c66e818afc67d"
   },
   "source": [
    "**Introducing Lambda Functions** Python allows you to write short functions in one line using the *lambda* keyword with a variable and a \":\". \n",
    "Below we will transform the abstract column into a new one that we call tokenized compressing our preprocessing pipeline into 3 lines\n",
    "\n",
    "We combine our lambda functions with the Pandas method \"map\" that apply this function to every row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "942d9c5e70b1687586fa96199869b9efd2f18448"
   },
   "outputs": [],
   "source": [
    "# Tokenize each abstract\n",
    "abstracts['tokenized'] = abstracts['Abstract'].map(lambda t: word_tokenize(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0b6bea9883070e61c0720cfbc88b103eac3bbc87"
   },
   "outputs": [],
   "source": [
    "# lowecase, strip and ensure it's words\n",
    "abstracts['tokenized'] = abstracts['tokenized'].map(lambda t: [word.lower().strip() for word in t if word.isalpha()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "626728fdb135880294e642354de4b6294a19faa2"
   },
   "outputs": [],
   "source": [
    "# lemmarize and remove stopwords\n",
    "abstracts['tokenized'] = abstracts['tokenized'].map(lambda t: [wordnet_lemmatizer.lemmatize(word) for word in t if word not in stopwords_en])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5eb21f7b5efaa13386272b8b58c6ad6c659385a5"
   },
   "source": [
    "Sure, one could do so much more to pre-process. We could try to identify bi-grams, remove prepositions, verbs etc. But already this brings us rather far.\n",
    "\n",
    "Now we will dive into Gensim further transform our abstracts using more advanced techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "653721ce6f0a05e23414748847d59caa9ca52eeb"
   },
   "outputs": [],
   "source": [
    "# We start by importing and initializing a Gensim Dictionary. \n",
    "# The dictionary will be used to map between words and IDs\n",
    "\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Create a Dictionary from the articles: dictionary\n",
    "dictionary = Dictionary(abstracts['tokenized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "94ced651e5d40ae9a24c539fa4487f69e840c5bf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# And this is how you can map back and forth\n",
    "# Select the id for \"firm\": firm_id\n",
    "firm_id = dictionary.token2id.get(\"firm\")\n",
    "\n",
    "# Use computer_id with the dictionary to print the word\n",
    "print(dictionary.get(firm_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c374d70d9fdbb459fad0361fea05b132a6693902"
   },
   "outputs": [],
   "source": [
    "# Create a Corpus: corpus\n",
    "# We use a list comprehension to transform our abstracts into BoWs\n",
    "corpus = [dictionary.doc2bow(abstract) for abstract in abstracts['tokenized']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1c4a1813e92abb4e9ec81c771e25626fcb7bef9b"
   },
   "outputs": [],
   "source": [
    "# Print the first 10 word ids with their frequency counts from the fifth document\n",
    "print(corpus[10][:10])\n",
    "\n",
    "# This is the same what we did before when we were counting words with the Counter (just in big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4a97a3a80ae25869dc0d9f6ea7524cf2582b156d"
   },
   "outputs": [],
   "source": [
    "# Sort the doc for frequency: bow_doc\n",
    "bow_doc = sorted(corpus[10], key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 words of the document alongside the count\n",
    "for word_id, word_count in bow_doc[:10]:\n",
    "    print(dictionary.get(word_id), word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1fc4ebeb8c1c3c75ed46ad9ca9bae2197ee1bb8a"
   },
   "source": [
    "#### TF-IDF - Term Frequency - Inverse Document Frequency\n",
    "\n",
    "A token is importan for a document if appears very often\n",
    "A token becomes less important for comparaison across a corpus if it appears all over the place in the corpus\n",
    "\n",
    "*Innovation* in a corpus of abstracts talking about innovation is not that important\n",
    "\n",
    "\\begin{equation*}\n",
    "w_{i,j} = tf_{i,j}*log(\\frac{N}{df_i})\n",
    "\\end{equation*}\n",
    "\n",
    "- $w_{i,j}$ = the TF-IDF score for a term i in a document j\n",
    "- $tf_{i,j}$ = number of occurence of term i in document j\n",
    "- $N$ = number of documents in the corpus\n",
    "- $df_i$ = number of documents with term i\n",
    "\n",
    "\n",
    "We will use TF-IDF to transform our corpus. However, first we need to fir the TF-IDF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ede4e1c084f8b940f6d31d9fb43ba1f2ac1c4efa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import the TfidfModel from Gensim\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "# Create and fit a new TfidfModel using the corpus: tfidf\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "tfidf_weights = tfidf[corpus[10]]\n",
    "\n",
    "# Print the first five weights\n",
    "print(tfidf_weights[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4f2b0d18ad3409277e422b7c3b52e00efa13cc6b"
   },
   "outputs": [],
   "source": [
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 weighted words\n",
    "for term_id, weight in sorted_tfidf_weights[:10]:\n",
    "    print(dictionary.get(term_id), weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b95599dde67a93a9489ae3210ddeb7bd2be0b8d7"
   },
   "outputs": [],
   "source": [
    "# Now we can transform the whole corpus\n",
    "tfidf_corpus = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "be7823cd2aa87c0169087d734a9bcac2e8cce604"
   },
   "source": [
    "The transformed corpus is much more interesting in terms of analysis than the pure bag of words representation. In fact, you could transform it now into a matrix and perform clustering and other unsupervised machine learning.\n",
    "\n",
    "![surprise](http://www.jaclynfriedman.com/wp-content/uploads/2018/06/giphy-23.gif)\n",
    "\n",
    "**Surprise**: This is exactly what topic modelling is about! Algorithms like LSI are closely related to PCA, NMF and SVD.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "102b9e0b32962d45d9aad9140bb71276cd6ef20e"
   },
   "outputs": [],
   "source": [
    "# Just like before, we import the model\n",
    "from gensim.models.lsimodel import LsiModel\n",
    "\n",
    "# And we fir it on the tfidf_corpus pointing to the dictionary as reference and the number of topics.\n",
    "# In more serious settings one would pick between 300-400\n",
    "lsi = LsiModel(tfidf_corpus, id2word=dictionary, num_topics=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5fa0621becb8e147678e34cb9f4efbba51cc9814"
   },
   "outputs": [],
   "source": [
    "# Once the model is ready, we can inspect the topics\n",
    "lsi.show_topics(num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d1dbb565f60940f748829f0b9ee22ed0a726c60e"
   },
   "outputs": [],
   "source": [
    "# And just as before, we can use the trained model to transform the corpus\n",
    "lsi_corpus = lsi[tfidf_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f027f1528a165449373d9cba06dd19f8f13b9438"
   },
   "source": [
    "At this point, our corpus is a document-topic matrix. in corpus-format. We can create a full matrix using the built in MatrixSimilarity function (which is actually used for similarity-queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "33e7821a966f95c5ace9ecda6b62c73030e9dac2"
   },
   "outputs": [],
   "source": [
    "# Load the MatrixSimilarity\n",
    "from gensim.similarities import MatrixSimilarity\n",
    "\n",
    "# Create the document-topic-matrix\n",
    "document_topic_matrix = MatrixSimilarity(lsi_corpus)\n",
    "document_topic_matrix = document_topic_matrix.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cda1cb6f1cd38fd85adf55625760e88584c8a910"
   },
   "outputs": [],
   "source": [
    "# Let's identify some clusters in our corpus\n",
    "\n",
    "# We import KMeans form the Sklearn library\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Instatiate a model with 4 clusters\n",
    "kmeans = KMeans(n_clusters=10)\n",
    "\n",
    "# And fit it on our matrix\n",
    "kmeans.fit(document_topic_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b3fbbcd99f9e519f360b5b5ac99179c9cfd2ba49"
   },
   "outputs": [],
   "source": [
    "# Let's annotate our abstracts with the assigned cluster number\n",
    "abstracts['cluster'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4cf3ae5b1969c7e9befc562cf8c35e0ce99c59ee"
   },
   "outputs": [],
   "source": [
    "# We can try to visualize our documents using TSNE - an approach for visualizing high-dimensional data\n",
    "\n",
    "# Import the module first\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# And instantiate\n",
    "tsne = TSNE()\n",
    "\n",
    "# Let's try to boil down the 100 dimensions into 2\n",
    "visualization = tsne.fit_transform(document_topic_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "afc8f04114ee4d3fb9e67ad34538b585a8fb210c"
   },
   "outputs": [],
   "source": [
    "# Import plotting library\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "df2b9bc7fb4b96e4113df0b02b2a35ebe7b67935"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "sns.scatterplot(visualization[:,0],visualization[:,1], data = abstracts, palette='RdBu', hue=abstracts.cluster, legend='full')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f49c022656a377a23e7a6a8bfa2926caf592ed22"
   },
   "source": [
    "Now let's explore the different clusters. For that we will look at the titles. We could do it \"manually\" but why not using NLP for that, too.\n",
    "We will preprocess the titles, just as we did witht he abstracts and then use TF-IDF of the title-token-sum of each cluster to see which tokens are most important in which cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5845c81b704fe13d6a6d3660afc59fc90d37ecd2"
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "abstracts['title_tok'] = abstracts['Title'].map(lambda t: word_tokenize(t))\n",
    "abstracts['title_tok'] = abstracts['title_tok'].map(lambda t: [word.lower().strip() for word in t if word.isalpha()])\n",
    "abstracts['title_tok'] = abstracts['title_tok'].map(lambda t: [wordnet_lemmatizer.lemmatize(word) for word in t if word not in stopwords_en])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4c07f5d756af5c2809895780425b0c7309ea31cf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Collectiong\n",
    "\n",
    "Cluster = 2\n",
    "\n",
    "cluster_titles = []\n",
    "for x in abstracts[abstracts['cluster'] == Cluster]['title_tok']:\n",
    "    cluster_titles.extend(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6a1bf24c51b440d52db577558546346f3e54970b"
   },
   "outputs": [],
   "source": [
    "# Transfortm into tf_idf format\n",
    "titles_tfidf = tfidf[dictionary.doc2bow(cluster_titles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f85bab2123461a51d3a336fac7b0e786441a1a5f"
   },
   "outputs": [],
   "source": [
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "titles_tfidf = sorted(titles_tfidf, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 weighted words\n",
    "for term_id, weight in titles_tfidf[:20]:\n",
    "    print(dictionary.get(term_id), weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
