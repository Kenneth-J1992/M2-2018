{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# Natural Language Processing (NLP) in Python\n",
    "## An introduction to key concepts and techniques\n",
    "\n",
    "Exercise notebook.\n",
    "\n",
    "- basic string manipulation\n",
    "- tokens and tokenization + some preprocessing\n",
    "- the Bag-of-Words model\n",
    "- topic modeling (and its close relation to dimensionality reduction / unsupervised machine learning)\n",
    "- text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8e17b1aa00a18ee6658bbbebed214f55545bb2fe"
   },
   "source": [
    "### Exercise 1: Basic string manimulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bc74dc66168ce914fb2fa57030be8d04c5f8c786"
   },
   "outputs": [],
   "source": [
    "text = \"\"\"Latanya Sweeney and Nick Diakopoulos pioneered the study of misbehavior in Google systems (Sweeney, 2013; Diakopoulos, 2013; Diakopoulos, 2016). Their work exposed instances of algorithmic defa- mation in Google searches and ads. Diakopoulos discussed a canonical example of such algorithmic defamation in which search engine auto- completion routines, fed a steady diet of historical user queries, learn to make incorrect defamatory or bigoted associations about people or groups of people.10 Sweeney showed that such learned negative associa- tions affect Google’s targeted ads. In her example, just searching for certain types of names led to advertising for criminal justice services, such as bail bonds or criminal record checking. Diakopoulos’s exam- ples included consistently defamatory associations for searches related to transgender issues.\n",
    "Studies like Sweeney’s and Diakopoulos’s are archetypes in the growing field of data and algorithmic journalism. More news and research articles chronicle the many missteps of the algorithms that affect different parts of our lives, online and off. IBM’s Jeopardy- winning AI, Watson, famously had to have its excessive swearing habit corrected after its learning algorithms ingested some unsavory data (Madrigal, 2013). There have also been reports on the effects of Waze’s traffic routing algorithms on urban traffic patterns (Bradley, 2015). One revealing book describes the quirks of the data and algorithms underlying the popular OkCupid dating service (Rudder, 2014). More recently, former Facebook contractors revealed that Facebook’s news feed trend algorithm was actually the result of subjective input from a human panel (Tufekci, 2016).\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0004a9e91bad4f3b6acdfcf8421a38407f1c4fda"
   },
   "outputs": [],
   "source": [
    "# We can split the text-chunk into something like sentences.\n",
    "split_text = text.split(***)\n",
    "print(split_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "82694ae57e7d801b0d83579e01756495efba4dc4"
   },
   "outputs": [],
   "source": [
    "# print out the first sentence\n",
    "\n",
    "sentence_1 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "52ce4d42879932296721af6bf57d2440d1e3c628"
   },
   "outputs": [],
   "source": [
    "# Let's create tokens from this sentence\n",
    "tokens_sentence_1 = [word for word in ****.split(****)]\n",
    "print(tokens_sentence_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "53bfd1e2120868b423c0c4a99ac64f4a8634bd2f"
   },
   "outputs": [],
   "source": [
    "# Let's lowercase all these tokens and clean up the \\n (new line command)\n",
    "\n",
    "\n",
    "tokens_sentence_1_lower = [word.****().****() for word in sentence_1.split(****)]\n",
    "print('### OUTPUT1 ###')\n",
    "print(tokens_sentence_1_lower)\n",
    "print('\\n')\n",
    "    \n",
    "# Also we will replace \"()\" as well as make sure that only words lend in our list\n",
    "tokens_sentence_1_lower = [word.replace('(','').replace(***,***) for word in tokens_sentence_1_lower if word.isalpha()]\n",
    "\n",
    "print('### OUTPUT2 ###')\n",
    "print(tokens_sentence_1_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f7a13cd3b9cbe888d821381c6fee88684121c077"
   },
   "outputs": [],
   "source": [
    "# Removing stopwords\n",
    "\n",
    "stopwords_en = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', \n",
    "                'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \n",
    "                \"you'd\", 'your', 'yours', 'yourself', 'yourselves', \n",
    "                'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', \n",
    "                'hers', 'herself', 'it', \"it's\", 'its', 'itself', \n",
    "                'they', 'them', 'their', 'theirs', 'themselves', 'what', \n",
    "                'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', \n",
    "                'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', \n",
    "                'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', \n",
    "                'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', \n",
    "                'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', \n",
    "                'between', 'into', 'through', 'during', 'before', 'after', 'above', \n",
    "                'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', \n",
    "                'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', \n",
    "                'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', \n",
    "                'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', \n",
    "                'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', \n",
    "                'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', \n",
    "                'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \n",
    "                \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \n",
    "                \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", \n",
    "                'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \n",
    "                'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "742adfae8302f83863195cafbc146a3b9b17c913"
   },
   "outputs": [],
   "source": [
    "tokens_sentence_1_clean = [word for word in ***** if word not in *****]\n",
    "print(tokens_sentence_1_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3f145224681c353d8a3331c93746d9e645b77e16"
   },
   "source": [
    "### Exercise 2: Using advanced tokenizers from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c1af2c20a5cb4034e72d58ff2987081411f39804"
   },
   "outputs": [],
   "source": [
    "# Tokenizing sentences\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Tokenizing words\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenizing Tweets!\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9615cbbca8db92f640e15175d22066042e25f117"
   },
   "outputs": [],
   "source": [
    "# Let's get our stences.\n",
    "# Note that the full-stops at the end of each sentence are still there\n",
    "sentences = sent_tokenize(****)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "873bb1337316eb0444aa1056376b0c34ecc427af"
   },
   "outputs": [],
   "source": [
    "# Use word_tokenize to tokenize the first sentence: tokenized_sent\n",
    "tokenized_sent = word_tokenize(****)\n",
    "\n",
    "# Make a set of unique tokens in the entire text: unique_tokens\n",
    "unique_tokens = set(****(****))\n",
    "print(unique_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: 3 Years of Donald T. on Twitter\n",
    "\n",
    "In this exercise you will explore Trump's hashtag use in the years 2016 - 2018\n",
    "The data is obtained from: https://github.com/bpb27/trump_tweet_data_archive/\n",
    "It will be loaded into a Pandas Dataframe (~Table), which will be time-indexed.\n",
    "The tweets for 2016 are extracted. Your job is to tokenize the tweets using the tweet-tokenizer and to select all used hashtags into one long list.\n",
    "Then use a counter to find the most common hashtags for the different years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The preprocessing is donw for you\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "trump_tweets = pd.read_json('https://github.com/SDS-AAU/M2-2018/raw/master/input/trump_twitter.json')\n",
    "trump_tweets.set_index(pd.to_datetime(trump_tweets['created_at']), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time-indexing let's us perform neat things such as resampling\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "trump_tweets.resample('W').favorite_count.mean().plot()\n",
    "trump_tweets.resample('W').retweet_count.mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we extract the tweets for year 2016. Can you also extract those for 2017 and 2018\n",
    "trump_tweets_2016 = trump_tweets[trump_tweets.index.year == 2016]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quick lock at the created Dataframe\n",
    "trump_tweets_2018.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can you find out the right column for the actual tweets?\n",
    "tweets = trump_tweets_2016['*****']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4d69a001609cec5a10b954b39326aa423b7c7a63",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We can use the tweet tokenizer to parse these tweets:\n",
    "\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "# parse the tweets\n",
    "tweets_tokenized = [tknzr.****(tweet) for tweet in ****]\n",
    "\n",
    "# print out the 10 first tweets in parsed\n",
    "print(****[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "681324b86db3e7550959539dc6de059e82151685"
   },
   "outputs": [],
   "source": [
    "# Get out all hashtags using loops\n",
    "\n",
    "# create an empty list: hashtags\n",
    "**** = []\n",
    "\n",
    "# Filter hashtags\n",
    "for tweet in tweets_tokenized:\n",
    "    ****.extend([word for word in tweet if *****.startswith('*****')])\n",
    "    \n",
    "# Print out the first 20 hashtags to check\n",
    "print(****[:**])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let't import the counter function\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count all hashtags\n",
    "hashtags_counter = ****(****)\n",
    "\n",
    "# create an object and print out the most common 10 hashtags\n",
    "most_common_10 = hashtags_counter.*****(**)\n",
    "print(most_common_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the object is a list of tupels where the first value is always the hashtag and the second the number how often it appeared in the course of a year.\n",
    "\n",
    "**Now let's bring everything down into compact functional form and rerun it for all years**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a (a bit clunky but easy to read) function \n",
    "# that picks out the top10 hashtags for 1 year (performing the above)\n",
    "\n",
    "def pick_top_10(year):\n",
    "    tweet_df = trump_tweets[trump_tweets.index.year == year]\n",
    "    tweets = tweet_df['text']\n",
    "    tweets_tokenized = [tknzr.tokenize(tweet) for tweet in tweets]\n",
    "    hashtags = []\n",
    "    for tweet in tweets_tokenized:\n",
    "        hashtags.extend([word for word in tweet if word.startswith('#')])\n",
    "        hashtags_counter = Counter(hashtags)\n",
    "    return dict(hashtags_counter.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [2015****]\n",
    "\n",
    "\n",
    "top10 = []\n",
    "\n",
    "for **** in ****:\n",
    "    top_10.****(pick_top_10(***))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f20c9237885e1dff42c95e37fc916619faa95f64"
   },
   "source": [
    "### Exercise 4: Going beyond hashtags: Mr. Trump continued\n",
    "\n",
    "![trump](https://media1.tenor.com/images/9cdcdb12c0c4c6895f61c614273588f0/tenor.gif?itemid=4814164)\n",
    "\n",
    "Let's ask python to read all Trump tweets and perhaps we can find some patterns and clusters using Gensim, TF-IDF and topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e806c007d26ca17e3d9c7c257f31154ce1fc8d0e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_en = stopwords.words('english')\n",
    "\n",
    "# Let's import a lemmatizer from NLTK and try how it works\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "942d9c5e70b1687586fa96199869b9efd2f18448"
   },
   "outputs": [],
   "source": [
    "# We already imported the data above and can use it right away\n",
    "\n",
    "# Tokenize each tweet\n",
    "trump_tweets['tokenized'] = ****['text'].map(lambda t: tknzr.****(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0b6bea9883070e61c0720cfbc88b103eac3bbc87"
   },
   "outputs": [],
   "source": [
    "# lowecase, strip and ensure we only include words\n",
    "trump_tweets['tokenized'] = trump_tweets['tokenized'].map(\n",
    "    lambda t: [word.lower().strip() for word in t if word.****()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "626728fdb135880294e642354de4b6294a19faa2"
   },
   "outputs": [],
   "source": [
    "# lemmarize and remove stopwords\n",
    "trump_tweets['tokenized'] = trump_tweets['tokenized'].map(\n",
    "    lambda t: [wordnet_lemmatizer.*****(word) for word in t \n",
    "               if word not in *****])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check\n",
    "trump_tweets['tokenized'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5eb21f7b5efaa13386272b8b58c6ad6c659385a5"
   },
   "source": [
    "Sure, one could do so much more to pre-process. We could try to identify bi-grams, remove prepositions, verbs etc. But already this brings us rather far.\n",
    "\n",
    "Now we will dive into Gensim further transform our abstracts using more advanced techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "653721ce6f0a05e23414748847d59caa9ca52eeb"
   },
   "outputs": [],
   "source": [
    "# We start by importing and initializing a Gensim Dictionary. \n",
    "# The dictionary will be used to map between words and IDs\n",
    "\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Create a Dictionary from the articles: dictionary\n",
    "dictionary = Dictionary(*****['tokenized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "94ced651e5d40ae9a24c539fa4487f69e840c5bf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# And this is how you can map back and forth\n",
    "# Select the id for \"hillary\": hillary_id\n",
    "hillary_id = dictionary.token2id.get(*****)\n",
    "\n",
    "# Use computer_id with the dictionary to print the word\n",
    "print(dictionary.get(*****))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c374d70d9fdbb459fad0361fea05b132a6693902"
   },
   "outputs": [],
   "source": [
    "# Create a Corpus: corpus\n",
    "# We use a list comprehension to transform our abstracts into BoWs\n",
    "corpus = [dictionary.****(tweet) for tweet in ******]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ede4e1c084f8b940f6d31d9fb43ba1f2ac1c4efa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import the TfidfModel from Gensim\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "# Create and fit a new TfidfModel using the corpus: tfidf\n",
    "tfidf = TfidfModel(****)\n",
    "\n",
    "# Now we can transform the whole corpus\n",
    "tfidf_corpus = tfidf[****]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "102b9e0b32962d45d9aad9140bb71276cd6ef20e"
   },
   "outputs": [],
   "source": [
    "# Just like before, we import the model\n",
    "from gensim.models.lsimodel import LsiModel\n",
    "\n",
    "# Fit a lsi model: lsi using the tfidf transformed corpus as input\n",
    "lsi = LsiModel(****, id2word=****, num_topics=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5fa0621becb8e147678e34cb9f4efbba51cc9814"
   },
   "outputs": [],
   "source": [
    "# Inspect the first 10 topics\n",
    "lsi.****(num_topics=***)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d1dbb565f60940f748829f0b9ee22ed0a726c60e"
   },
   "outputs": [],
   "source": [
    "# Create a transformed corpus using the lsi model from the tfidf corpus: lsi_corpus\n",
    "**** = lsi[****]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f027f1528a165449373d9cba06dd19f8f13b9438"
   },
   "source": [
    "At this point, our corpus is a document-topic matrix. in corpus-format. We can create a full matrix using the built in MatrixSimilarity function (which is actually used for similarity-queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "33e7821a966f95c5ace9ecda6b62c73030e9dac2"
   },
   "outputs": [],
   "source": [
    "# Load the MatrixSimilarity\n",
    "from gensim.similarities import MatrixSimilarity\n",
    "\n",
    "# Create the document-topic-matrix\n",
    "document_topic_matrix = ***(****)\n",
    "document_topic_matrix = document_topic_matrix.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cda1cb6f1cd38fd85adf55625760e88584c8a910"
   },
   "outputs": [],
   "source": [
    "# Let's identify some clusters in our corpus\n",
    "\n",
    "# We import KMeans form the Sklearn library\n",
    "from sklearn.cluster import ****\n",
    "\n",
    "# Instatiate a model with 4 clusters\n",
    "kmeans = ****(****=**)\n",
    "\n",
    "# And fit it on our document-topic-matrix\n",
    "kmeans.fit(******)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b3fbbcd99f9e519f360b5b5ac99179c9cfd2ba49"
   },
   "outputs": [],
   "source": [
    "# Let's annotate our abstracts with the assigned cluster number\n",
    "trump_tweets['cluster'] = kmeans.*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4cf3ae5b1969c7e9befc562cf8c35e0ce99c59ee"
   },
   "outputs": [],
   "source": [
    "# We can try to visualize our documents using TSNE - \n",
    "# an approach for visualizing high-dimensional data\n",
    "\n",
    "# Import the module first\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# And instantiate\n",
    "tsne = ****()\n",
    "\n",
    "# Let's try to boil down the 100 dimensions into 2\n",
    "visualization = tsne.fit_transform(******)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "afc8f04114ee4d3fb9e67ad34538b585a8fb210c"
   },
   "outputs": [],
   "source": [
    "# Import the plotting library\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "df2b9bc7fb4b96e4113df0b02b2a35ebe7b67935"
   },
   "outputs": [],
   "source": [
    "# Plot the trump_tweet map\n",
    "plt.figure(figsize=(15,15))\n",
    "sns.lmplot(visualization[:,0],visualization[:,1], \n",
    "           data = trump_tweets, palette='RdBu', \n",
    "           hue=trump_tweets.cluster, legend='full')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5:  Explore the content of the created clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4c07f5d756af5c2809895780425b0c7309ea31cf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Collectiong\n",
    "\n",
    "# Select a cluster e.g. 1\n",
    "Cluster = 1\n",
    "\n",
    "# Create an empty cluster token list: cluster_tweets\n",
    "**** = []\n",
    "\n",
    "# Create a loop which iterates over all tokenized tweets in the \n",
    "# trump_tweets dataframe and extends the created list with them\n",
    "for x in ****[trump_tweets['cluster'] == Cluster]['tokenized']:\n",
    "    cluster_tweets.****(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6a1bf24c51b440d52db577558546346f3e54970b"
   },
   "outputs": [],
   "source": [
    "# Transfortm the selected tweets using the tfidf model\n",
    "\n",
    "tweets_in_cluster_tfidf = tfidf[****.doc2bow(****)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f85bab2123461a51d3a336fac7b0e786441a1a5f"
   },
   "outputs": [],
   "source": [
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "# this has been done for you\n",
    "tweets_in_cluster_tfidf = sorted(tweets_in_cluster_tfidf, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 10 weighted words\n",
    "for term_id, weight in titles_tfidf[:10]:\n",
    "    print(dictionary.***(term_id), weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Predicting popular tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_tweets['is_retweet'] = trump_tweets['is_retweet'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = trump_tweets[trump_tweets['is_retweet'] == False].favorite_count.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_selector = trump_tweets[trump_tweets['is_retweet'] == False].iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = document_topic_matrix[x_selector,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set (since we have a new output variable)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's fit a simple linear regression model\n",
    "\n",
    "from sklearn.ense import LinearRegression\n",
    "\n",
    "regressor = LinearRegression()\n",
    "\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's fit a simple linear regression model\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "regressor = GradientBoostingRegressor()\n",
    "\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.regplot(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
